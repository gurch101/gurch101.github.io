{"pageProps":{"post":{"id":"system-design-interview","title":"System Design Interview","category":"book summary","description":"System Design Interview Summary","type":"notes","date":"Sat Oct 09 2021","contentHtml":"<h3>Chapter 1: Scaling from 1 to a million users</h3>\n<ul>\n<li>start with a single server to host static assets, app server, database server</li>\n<li>vertically scale until you can't</li>\n<li>split db and app server so they can be scaled independently\n<ul>\n<li>nosql = low latency, unstructured, massive amounts of data</li>\n</ul>\n</li>\n<li>horizontally scale - use a load balancer\n<ul>\n<li>improved availability</li>\n<li>stateful architecture requires sticky sessions</li>\n<li>stateless - more scalable -> store user state in redis/nosql db/relational db</li>\n</ul>\n</li>\n<li>failover/redundancy - db replication with one master db for writes and many read nodes</li>\n<li>add a cache for data that is read frequently but updated infrequently\n<ul>\n<li>considerations: expiration, what to cache, consistency, eviction (what to do when full - LRU)</li>\n</ul>\n</li>\n<li>use a CDN\n<ul>\n<li>considerations: cost (only cache frequently used content), expiry, failover, invalidation</li>\n</ul>\n</li>\n<li>use multiple data centers and a geoDNS - routes to server closest to user\n<ul>\n<li>considerations: automated deployments, monitoring, replication, immutable infrastructure</li>\n</ul>\n</li>\n<li>use message queues for long running tasks</li>\n<li>sharding the db to scale horizontally\n<ul>\n<li>considerations: resharding data when shards need to be split further, celebrity problem (certain users are more popular/active than others), leads to denormalization to prevent joins</li>\n</ul>\n</li>\n<li>split tiers into separate services</li>\n</ul>\n<h3>Chapter 2: Back-Of-The-Envelope Estimation</h3>\n<p>2^10 = 1024 = 1 kb - thousands\n2^20 = 1048576 = 1 mb - millions\n2^30 = 1gb - billions\n2^40 = 1 tb - trillions\n2^50 = 1 pb - quadrillions</p>\n<p>• Memory is fast but the disk is slow.\n• Avoid disk seeks if possible.\n• Simple compression algorithms are fast.\n• Compress data before sending it over the internet if possible.\n• Data centers are usually in different regions, and it takes time to send data between them.</p>\n<p>99% availability = down 3 days/year\n99.9% availability = down 8 hours/year</p>\n<p>Example: Estimate Twitter QPS and storage requirements\nPlease note the following numbers are for this exercise only as they are not real numbers\nfrom Twitter.\nAssumptions:\n• 300 million monthly active users.\n• 50% of users use Twitter daily.\n• Users post 2 tweets per day on average.\n• 10% of tweets contain media.\n• Data is stored for 5 years.\nEstimations:\nQuery per second (QPS) estimate:\n• Daily active users (DAU) = 300 million * 50% = 150 million\n• Tweets QPS = 150 million * 2 tweets / 24 hour / 3600 seconds = ~3500\n• Peek QPS = 2 * QPS = ~7000\nWe will only estimate media storage here.\n• Average tweet size:\n• tweet_id 64 bytes\n• text\n140 bytes\n• media\n1 MB\n• Media storage: 150 million * 2 * 10% * 1 MB = 30 TB per day\n• 5-year media storage: 30 TB * 365 * 5 = ~55 PB</p>\n<h3>Chapter 3: A Framework For System Design Interviews</h3>\n<ul>\n<li>deliberately ambiguous</li>\n<li>purpose is to demonstrate design skill, defend design choices, and respond to feedback</li>\n<li>interviewer is looking for ability to collaborate, resolve ambiguity, work under pressure</li>\n</ul>\n<p>step 1: understand the problem and establish design scope</p>\n<p>web/mobile/both?\nwhat features do we need to build?\nwhat are the goals?\nhow many users does the product have?\nhow fast does the company anticipate the scale up?\nwhat is the tech stack?\nwhat existing services can you leverage to simplify the design</p>\n<p>step 2: propose high-level design</p>\n<p>ask for feedback - treat interviewer as a teammate\nthink out loud\ngo through use cases\nsketch out lines and boxes</p>\n<p>step 3: design deep dive</p>\n<p>ask about which areas to prioritize</p>\n<p>step 4: answer questions/wrap up</p>\n<p>identify bottlenecks, potential improvements\ntalk about errors - server failure, network loss\nmonitoring/metrics/roll out</p>\n<h3>Chapter 4: Design a Rate Limiter</h3>\n<p>Used to control rate of traffic sent by a client or service by limit the number of client requests allowed to be sent over a specified period.</p>\n<p>Goals: prevent DOS, reduce cost</p>\n<p>Step 1: Understand the problem and establish design scope.</p>\n<p>client side or server side?\nthrottle mechanism - IP, user id, other properties? system wide?\nscale? distributed?\ndifferent limiters for different endpoints?</p>\n<p>Step 2: Propose high-level design</p>\n<p>Start with client/server, then add separate services.</p>\n<p>Algorithms:</p>\n<ul>\n<li>\n<p>Token bucket algorithm: put tokens into a bucket periodically up to some capacity. Remove a token for each request.</p>\n<ul>\n<li>pros: easy to implement, memory efficient, allows bursty traffic. Cons: difficult to tune bucket size/refill rate</li>\n</ul>\n</li>\n<li>\n<p>Leaking bucket algorithm: put requests on a queue - if queue is full, drop request, else process request at a fixed rate</p>\n<ul>\n<li>pros: memory efficient, good for cases where a stable outflow rate is needed</li>\n<li>cons: not good for bursty traffic</li>\n</ul>\n</li>\n<li>\n<p>Fixed window counter algorithm: divide time into fix-sized time windows and assign a counter to each window. Increment counter by one for each request. Once the counter reaches a threshold, drop requests until new window starts.</p>\n</li>\n<li>\n<p>Sliding window algorithm: remove all timestamps older than start of current time window, add timestamp for new request, if the log size is the same or lower than allowed count, accept request.</p>\n<ul>\n<li>pros: very accurate, cons: consumes memory because timestamps are kept</li>\n</ul>\n</li>\n<li>\n<p>redis has INCR/EXPIRE to increment and timeout a counter.</p>\n</li>\n</ul>\n<p>Step 3: Design Deep-Dive</p>\n<p>maintain rules in configuration files, return HTTP 429 if rate is exceeded. Return headers to clients to provide number of remaining requests within windows. Possibly queue excess requests for later processing on message queue.</p>\n<p>Possible problems</p>\n<ul>\n<li>race conditions when reading/updating counters - use locks</li>\n<li>synchronization issues in distributed environments where multiple caches are involved. Use sticky sessions or centralized data stores.</li>\n<li>performance - make service geographically distrubted and route to nearest server. Synchronize with eventual consistency.</li>\n<li>monitoring - gather analytics data to check whether limiter is effective</li>\n</ul>\n<p>Step 4: Wrap up</p>\n<ul>\n<li>hard vs soft rate limiting where you allow some requests to exceed threshold for a short period</li>\n<li>rate limit via iptables instead of app level</li>\n<li>design client with best practices - use client cache, graceful recovery from exceptions</li>\n</ul>\n<h3>Chapter 5: Design Consistent Hashing</h3>\n<p>If you have n cache servers, a common way to balance the load is to use the following hash method:</p>\n<div class=\"remark-highlight\"><pre class=\"language-serverIndex\"><code class=\"language-serverIndex\">undefined</code></pre></div>\n<p>Problem - if number of servers change, we get different server indices</p>\n<p>Consistent hashing: when hash table is resized, only k/n keys need to be remapped where k is the number of keys, and n is the number of slots. In contrast, in most traditional hash tables, a change in the number of array slots causes nearly all keys to be remapped.</p>\n<p>map servers and keys on to a ring using a uniformly distributed hash function. To find out which server a key is mapped to, go clockwise from the key position until the first server on the ring is found.</p>\n<p>problems: partition sizes can be different when servers are added/removed. Key distribution can be non-uniform.</p>\n<p>Solutions:</p>\n<ul>\n<li>represent each server with multiple virtual nodes so that each server is responsible for multiple partitions. More virtual nodes = more balanced distribution but this requires more storage to store data about each virtual node.</li>\n<li>new node: redistribute data by going anti-clockwise from new node until a server is found.</li>\n<li>removed node: go anti-clockwise from removed node until server is found and redistribute to next node</li>\n</ul>\n<p>used to deal with celebrity problem by making sure celebrity data is more evenly distributed.</p>\n"}},"__N_SSG":true}