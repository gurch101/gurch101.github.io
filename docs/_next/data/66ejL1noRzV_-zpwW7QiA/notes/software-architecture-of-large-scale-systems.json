{"pageProps":{"post":{"id":"software-architecture-of-large-scale-systems","title":"Software Architecture & Technology of Large-Scale Systems","category":"summary","description":"Udemy course notes","type":"notes","date":"Sat May 28 2022","contentHtml":"<p>Traditional three-tier architecture - frontend/web tier, application server/business logic, database server</p>\n<h3>Performance</h3>\n<p>A measure of how fast or responsive a system is under a given workload and given hardware.</p>\n<p>Every performance problem is the result of some queue building up somewhere. This queue builds up due to inefficient/slow processing, serial resource access (only one thing can access resource at a time ie low concurrency), or limited resource capacity.</p>\n<p>Efficiency</p>\n<ul>\n<li>resource utilization (IO/CPU)</li>\n<li>logic (algorithms, db queries)</li>\n<li>storage (db schema/indices, data structures)</li>\n<li>caching</li>\n</ul>\n<p>Concurrency</p>\n<ul>\n<li>hardware</li>\n<li>software</li>\n</ul>\n<p>Capacity</p>\n<ul>\n<li>CPU</li>\n<li>RAM</li>\n<li>Disk</li>\n<li>Network</li>\n</ul>\n<h5>Performance Objectives</h5>\n<p>Minimize request-response latency</p>\n<ul>\n<li>affects single user experience</li>\n<li>wait/idle time + processing time</li>\n</ul>\n<p>Maximize throughput</p>\n<ul>\n<li>rate of request processing</li>\n<li>affects number of users that a system can support</li>\n<li>decreasing latency and increasing capacity can increase throughput</li>\n</ul>\n<p>To visualize performance, plot request time (X) vs number of requests (Y). If there is a positive skew, the tail latency indicates requests are being queued.</p>\n<h3>Network transfer latency</h3>\n<ul>\n<li>network hops between potentially distant/unreliable servers</li>\n<li>TCP overhead - connection creation request/acknowledgement (SYN/ACK) is a round trip</li>\n<li>SSL overhead - TCP overhead + SSL hello/key exchange (2 more round trips)</li>\n</ul>\n<p>Approaches to minimize network latencies</p>\n<ul>\n<li>between client + web server (persistent connections - HTTP 1.1, static data caching)</li>\n<li>data format (gRPC/thrift for intranet/microservice communication)</li>\n<li>data compression</li>\n<li>SSL session caching</li>\n<li>session/data caching</li>\n<li>connection pooling</li>\n</ul>\n<h3>Memory access latency</h3>\n<ul>\n<li>GC execution</li>\n<li>Memory swaps when app memory exceeds physical memory</li>\n<li>finite buffer memory for db</li>\n</ul>\n<p>Approaches to minimize memory latencies</p>\n<ul>\n<li>avoid memory bloat</li>\n<li>use smallest heap as possible to minimize GC runtime</li>\n<li>multiple smaller processes are better than one large process</li>\n<li>use a purpose built GC algorithm (real-time GC for server, blocking but more efficient GC for batch processing)</li>\n<li>maximize buffer memory, normalize data, consider computation over storage (if value can be computed, compute it rather than store it)</li>\n</ul>\n<h3>Disk access latency</h3>\n<ul>\n<li>logging</li>\n<li>database</li>\n<li>web content (static files)</li>\n</ul>\n<p>Approaches to minimize disk latency</p>\n<ul>\n<li>web content caching (static files)</li>\n<li>async/batch logging (disadvantage - data loss)</li>\n<li>batch I/O</li>\n<li>query optimization</li>\n<li>db cache</li>\n<li>db indexes</li>\n<li>schema denormalization</li>\n<li>faster disk, using RAID to allow parallel reads</li>\n</ul>\n<h3>CPU latency</h3>\n<ul>\n<li>inefficient algorithms</li>\n<li>context switching - process state is saved/restored to/from memory/cpu when multiple processes are running on a computer. Also caused by I/O</li>\n</ul>\n<p>Approaches to minimize CPU latency</p>\n<ul>\n<li>async/batch I/O</li>\n<li>use a single threaded model (nginx, node.js - one thread handles request/response, other threads do I/O)</li>\n<li>thread pool should be sized appropriately to number of cores</li>\n<li>run processes in their own virtual env to ensure each process has dedicated CPU</li>\n</ul>\n"}},"__N_SSG":true}